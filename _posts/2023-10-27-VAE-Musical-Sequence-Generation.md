---
title: "Musical Sequence Generation in Eurorack Using a Variational Autoencoder"
published: false
---
# Musical Sequence Generation in Eurorack Using a Variational Autoencoder



## Variation Autoencoder Architecture

A variational autoencoder (VAE) to a type of neural network. A VAE learns to encode complex data into a lower-dimensional space (latent space) and then decode it back to the original data space. New data can then be generated by sampling from the latent space.

The input of the encoder is a fixed length sequence of boolean values which represet a fixed length sequence of musical notes. e.g. If notes are represented by points on a grid with pitch vertically and time horizontally, the input to the VAE is a flattened version of this.

![Input](/assets/images/2023-10-27-vae-musical-sequence-generation/input_encoding.drawio.png)

The VAE has an input for each note per tick of the clock, so the total number of inputs is notes x ticks.

Training is done by presenting the same input data as output data. The bottleneck in the VAE causes it to find a low dimensional representation of the input. In this case 3 numbers are all that are needed to represent a particular piece of input data.

![VAE](/assets/images/2023-10-27-vae-musical-sequence-generation/vae_training.drawio.png)

Once the VAE is trained the decoder part of the network is converted to tflite which is a format that we can use on hardware. The latent variables can be set directly from a potentiomer or control voltage input to generate a new sequence.

![Decoder](/assets/images/2023-10-27-vae-musical-sequence-generation/vae_inference.drawio.png)

The output sequence is then converted back into a time based sequence by splitting it up and fed to the outputs based on an external clock pulse.

### Percussion mapping

Percussion sequences are handled slightly differently to sequences of musical notes. Instead of using pitch we just map each instrument to a different row in the sequence. Because the number of trigger outputs is limited in hardware we map midi intrument numbers to smaller subset of intruments.



## Training

Training is done on a batch of midi files. The [xen_machinelearning](https://github.com/pigatron-industries/xen_machinelearning/blob/main/python/train_vae.ipynb) repository contains handy functions for filtering and converting midi files to the required sequence format.

Since the sequence length has to be a fixed size for the VAE we need to decide the length of each sequence to train it on. The midi files are cut up into sections of a fixed length, with a measure of music being a convenient length. We also want to filter the data to specific time signature to ensure the bar lengths we are training on are constant. e.g. We can filter all measures with a 4/4 time signature and encode them as 16 step sequences, each step representing 1/16th note.

``` python
from xen.training.SequenceVAETrainer import SequenceVAETrainer

paths = ["../../../ai/trainingdata/music/mutopia_guitar/"]
trainer = SequenceVAETrainer(modelPath="../models", modelName="mutopia_guitar_16")
trainer.loadSongDataset(paths, timesig='4/4', ticksPerQuarter=4, quartersPerMeasure=4, measuresPerSequence=1)
```

In the above code we are loading a directory of midi files and filtering it to 4/4 time signature, and encoding to sequences of 1 measure length, with 4 quarter notes per measure and 4 steps or ticks per quarter note, making a total sequence length of 16 (4 * 4 * 1).

If any of the sequences left after filtering contain notes which don't adhere the sequence constraints, then those measures will also be discarded. e.g. triplets cannot be represented by a sequence length of 16, if those were to be included then ticksPerQuarter would need to be 12, making the sequence length 12 * 4 * 1 = 48

Once the data is loaded the VAE model can be created with:

``` python
trainer.createModel(latentDim = 3, hiddenLayers = 2)
```

The latentDim is the number of dimensions in latent space which will correspond to the 3 inputs during inference. The hiddenLayers is the number of layers we want to add between the input and latent layers. The size of the hidden layers will be automatically calculated to produce a linear size decrease from input to latent layers.

All that remains is to train the model and save it:

``` python
trainer.train(batchSize = 32, epochs = 500, learning_rate = 0.005)
trainer.saveModel(quantize = None)
```

After training the model will be saved as a complete VAE (***_vae.h5) as well as a tflite file containing just the decoder to be used by microcontroller inference. 

## Inference on Microcontroler

TODO link to code to perform inference to test model

## Drum Mapping


